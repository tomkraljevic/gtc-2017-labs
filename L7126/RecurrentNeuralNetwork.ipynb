{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Recurrent Neural Networks\n",
    "\n",
    "First, let's start with a toy example. The dataset is comprised of two sentences. Let's train a simple recurrent neural network to predict words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 10 20:05:54 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   50C    P8    26W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import inspect\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "#import reader\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoded inputs\n",
      "[[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "shape of the input\n",
      "(2, 6, 10)\n",
      "reshaped input for training\n",
      "[[array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]]), array([[ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]), array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]]), array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "a=np.array([[1,2,3,4,5,0],[1,2,3,4,6,0]]) \n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())    \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        result=tf.nn.embedding_lookup(np.identity(10), a).eval()\n",
    "        example_input=sess.run([tf.unstack(result,6,1)])\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "print('one-hot encoded inputs')\n",
    "print(result)\n",
    "print('shape of the input')\n",
    "print(result.shape)\n",
    "print('reshaped input for training')\n",
    "print(example_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single layer RNN with LSTMs and train it with a toy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0  loss:  2.50325\n",
      "iteration:  25  loss:  0.728643\n",
      "iteration:  50  loss:  0.148765\n",
      "iteration:  75  loss:  0.121297\n",
      "iteration:  100  loss:  0.118705\n",
      "iteration:  125  loss:  0.117768\n",
      "iteration:  150  loss:  0.117226\n",
      "iteration:  175  loss:  0.116867\n",
      "iteration:  200  loss:  0.116615\n",
      "iteration:  225  loss:  0.11643\n",
      "iteration:  250  loss:  0.11629\n",
      "iteration:  275  loss:  0.116182\n",
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[2 2 7 7 4 4 5 6 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "plot_loss=[]\n",
    "num_hidden=24\n",
    "num_steps=6\n",
    "dict_length=8\n",
    "batch_size=2\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Make Variables\n",
    "variables_dict = {\n",
    "    \"weights1\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],stddev=1.0,dtype=tf.float32),name=\"weights1\"),\n",
    "    \"biases1\": tf.Variable(tf.truncated_normal([dict_length],stddev=1.0,dtype=tf.float32), name=\"biases1\")}\n",
    "\n",
    "\n",
    "# Create input data\n",
    "small_dict=['EOS','i','will','walk','the','dog','cat','run']\n",
    "X=np.array([[1,2,7,4,5,0],[1,2,3,4,6,0]],dtype=np.int32)  \n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), X) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "y=np.zeros((batch_size,num_steps),dtype=np.int32)\n",
    "y[:,:-1]=X[:,1:]\n",
    "y_one_hot=tf.unstack(tf.nn.embedding_lookup(np.identity(dict_length), y),num_steps,1) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "\n",
    "y_target_reshape=tf.reshape(y_one_hot,[batch_size*num_steps,dict_length])\n",
    "\n",
    "#Create our LSTM\n",
    "cell = tf.contrib.rnn.LSTMCell(num_units=num_hidden, state_is_tuple=True)\n",
    "\n",
    "outputs, last_states = tf.contrib.rnn.static_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float32,\n",
    "    inputs=tf.unstack(tf.to_float(X_one_hot),num_steps,1))\n",
    "\n",
    "output_reshape=tf.reshape(outputs, [batch_size*num_steps,num_hidden]) #[12==batch_size*num_steps,num_hidden==12]\n",
    "pred=tf.matmul(output_reshape, variables_dict[\"weights1\"]) +variables_dict[\"biases1\"]\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target_reshape))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())    \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)        \n",
    "        for i in range(300):\n",
    "            loss,_,y_target,y_pred,output=sess.run([cost,optimizer,y_target_reshape,pred,outputs])\n",
    "            plot_loss.append([loss])\n",
    "\n",
    "            if i% 25 ==0:\n",
    "                print(\"iteration: \",i,\" loss: \",loss)\n",
    "                \n",
    "        print(y_target)\n",
    "        print(np.argmax(y_pred,1))          \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence\n",
      "['i', 'will', 'run', 'the', 'dog', 'EOS']\n",
      "Predicted words\n",
      "['will', 'run', 'the', 'dog', 'EOS', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "#Lets look at one input data point at each step and its prediction\n",
    "print(\"Input Sentence\")\n",
    "print([small_dict[ind] for ind in X[0,:]])\n",
    "print(\"Predicted words\")\n",
    "print([small_dict[ind] for ind in np.argmax(y_pred[0::2],1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to increase the depth of our RNN. Let's train an RNN with 2 and 4 layers. What parameter do you need to set to change the number of layers in your RNN? For a hint look [here](#answer1 \"num_layers=2 or num_layer=4\").\n",
    "\n",
    "Where do you enter the dropout values for the RNN?  For a hint look [here](#answer2 \"lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell,input_keep_prob=input_keep_prob,output_keep_prob=output_keep_prob)\").                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0  loss:  2.62829\n",
      "iteration:  25  loss:  0.996458\n",
      "iteration:  50  loss:  0.19019\n",
      "iteration:  75  loss:  0.120728\n",
      "iteration:  100  loss:  0.117841\n",
      "iteration:  125  loss:  0.117149\n",
      "iteration:  150  loss:  0.116771\n",
      "iteration:  175  loss:  0.116522\n",
      "iteration:  200  loss:  0.116346\n",
      "iteration:  225  loss:  0.116215\n",
      "iteration:  250  loss:  0.116113\n",
      "iteration:  275  loss:  0.116032\n",
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[2 2 7 7 4 4 5 6 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Now let's try multiple layers \n",
    "plot_loss2=[]\n",
    "num_hidden=24\n",
    "num_steps=6\n",
    "dict_length=8\n",
    "batch_size=2\n",
    "num_layers=2\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Make Variables\n",
    "variables_dict = {\n",
    "    \"weights1\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],stddev=1.0,dtype=tf.float32),name=\"weights1\"),\n",
    "    \"biases1\": tf.Variable(tf.truncated_normal([dict_length],stddev=1.0,dtype=tf.float32), name=\"biases1\")}\n",
    "\n",
    "\n",
    "# Create input data\n",
    "small_dict=['EOS','i','will','walk','the','dog','cat','run']\n",
    "X=np.array([[1,2,7,4,5,0],[1,2,3,4,6,0]],dtype=np.int32)  \n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), X) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "y=np.zeros((batch_size,num_steps),dtype=np.int32)\n",
    "y[:,:-1]=X[:,1:]\n",
    "y_one_hot=tf.unstack(tf.nn.embedding_lookup(np.identity(dict_length), y),num_steps,1) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "y_target_reshape=tf.reshape(y_one_hot,[batch_size*num_steps,dict_length])\n",
    "\n",
    "input_keep_prob=1.0#input_keep_prob,\n",
    "output_keep_prob=1.0#output_keep_pro\n",
    "\n",
    "##################### Create a multilayer RNN ####################\n",
    "layer_cell=[]\n",
    "for _ in range(num_layers):\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_hidden, state_is_tuple=True)\n",
    "    lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell,\n",
    "                                          input_keep_prob=input_keep_prob,\n",
    "                                          output_keep_prob=output_keep_prob)\n",
    "    layer_cell.append(lstm_cell)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(layer_cell, state_is_tuple=True)\n",
    "outputs, last_states = tf.contrib.rnn.static_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float32,\n",
    "    inputs=tf.unstack(tf.to_float(X_one_hot),num_steps,1))\n",
    "\n",
    "output_reshape=tf.reshape(outputs, [batch_size*num_steps,num_hidden]) #[12==batch_size*num_steps,num_hidden==12]\n",
    "pred=tf.matmul(output_reshape, variables_dict[\"weights1\"]) +variables_dict[\"biases1\"]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target_reshape))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())    \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(300):\n",
    "            loss,_,y_target,y_pred,output=sess.run([cost,optimizer,y_target_reshape,pred,outputs])\n",
    "            plot_loss2.append([loss])\n",
    "            \n",
    "            if i% 25 ==0:\n",
    "                print(\"iteration: \",i,\" loss: \",loss)\n",
    "                \n",
    "        print(y_target)\n",
    "        print(np.argmax(y_pred,1))         \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the loss from our single and multi-layer RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2737fcc390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## import matplotlib.pyplot as plt\n",
    "plt.plot(plot_loss,'r.')\n",
    "plt.plot(plot_loss2,'b--')\n",
    "plt.legend([\"1-Layer LSTM\",\"Multi-Layer LSTM\"])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.title(\"Loss During Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an RNN with MSCOCO Captions \n",
    "Now we are going to try a slightly more complicated example. Let's use the [Microsoft Common Objects in Context](http://mscoco.org/) (MSCOCO) image captions to train an RNN. The cell below shows one way to read, format, and feed the data into TensorFlow. First, we will read the caption file, then we will remove the punctuation, and then train. Due to time constraints, we won't use the full dataset for this training. However, it would be easy to change this and train with more or the entire dataset. Can you see an easy way to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps=20\n",
    "## Read Training files\n",
    "with open(\"/data/mscoco/captions_train2014.json\") as data_file:\n",
    "         data=json.load(data_file)\n",
    "\n",
    "TotalNumberofCaptions=len(data['annotations'])\n",
    "\n",
    "sentences=[]\n",
    "\n",
    "##Create a list of all of the sentences.\n",
    "for i in range(TotalNumberofCaptions):\n",
    "        sentences+=[re.sub('[^A-Za-z0-9]+',' ',data['annotations'][i]['caption']).lower()]\n",
    "\n",
    "TotalWordList=[]\n",
    "for i in range(TotalNumberofCaptions):\n",
    "        TotalWordList+=re.sub('[^A-Za-z0-9]+',' ',data['annotations'][i]['caption']).lower().split()\n",
    "\n",
    "#Determine number of distint words \n",
    "distintwords=collections.Counter(TotalWordList)\n",
    "#Order words \n",
    "count_pairs = sorted(distintwords.items(), key=lambda x: (-x[1], x[0])) #ascending order\n",
    "words, occurence = list(zip(*count_pairs))\n",
    "DictionaryLength=occurence.index(4) #index for words that occur 4 times or less\n",
    "words=['PAD','UNK','EOS']+list(words[:DictionaryLength])\n",
    "word_to_id=dict(zip(words, range(len(words))))\n",
    "#Tokenize Sentence\n",
    "Tokenized=[]\n",
    "for full_words in sentences:\n",
    "        EmbeddedSentence=[word_to_id[word] for word in full_words.split() if word in word_to_id]+[word_to_id['EOS']]\n",
    "        #Pad sentences that are shorter than the number of steps \n",
    "        if len(EmbeddedSentence)<num_steps:\n",
    "            b=[word_to_id['PAD']]*num_steps\n",
    "            b[:len(EmbeddedSentence)]=EmbeddedSentence\n",
    "        if len(EmbeddedSentence)>num_steps:\n",
    "            b=EmbeddedSentence[:num_steps]\n",
    "        if len(b)==EmbeddedSentence:\n",
    "            b=EmeddedSentence\n",
    "        b=[word_to_id['UNK'] if x>=DictionaryLength else x for x in b] #turn all words used 4 times or less to 'UNK'\n",
    "        #print(b)\n",
    "        Tokenized+=[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a very clean and well decorated empty bathroom', u'a panoramic view of a kitchen and all of its appliances ', u'a blue and white bathroom with butterfly themed wall tiles ', u'a panoramic photo of a kitchen and dining room', u'a graffiti ed stop sign across the street from a red car ', u'a vandalized stop sign and a red beetle on the road', u'a bathroom with a border of butterflies and blue paint on the walls above it ', u'an angled view of a beautifully decorated bathroom ', u'the two people are walking down the beach ', u'a sink and a toilet inside a small bathroom ']\n",
      "[[3, 142, 508, 9, 619, 415, 276, 57, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 4027, 171, 5, 3, 61, 9, 317, 5, 155, 612, 2, 0, 0, 0, 0, 0, 0, 0, 0], [3, 59, 9, 21, 57, 8, 2922, 1963, 136, 1225, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 4027, 162, 5, 3, 61, 9, 461, 43, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 638, 6775, 148, 58, 241, 6, 24, 96, 3, 47, 128, 2, 0, 0, 0, 0, 0, 0, 0], [3, 2539, 148, 58, 9, 3, 47, 7524, 4, 6, 84, 2, 0, 0, 0, 0, 0, 0, 0, 0], [3, 57, 8, 3, 2984, 5, 5243, 9, 59, 1228, 4, 6, 486, 240, 27, 2, 0, 0, 0, 0], [14, 4659, 171, 5, 3, 3647, 415, 57, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 15, 18, 19, 55, 31, 6, 75, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 115, 9, 3, 82, 160, 3, 36, 57, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:10])\n",
    "print(Tokenized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 142, 508, 9, 619, 415, 276, 57, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[142, 508, 9, 619, 415, 276, 57, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Number of words in this dictionary  8768\n"
     ]
    }
   ],
   "source": [
    "############################################# Parameters #####################################################\n",
    "\n",
    "num_hidden=256\n",
    "num_steps=20\n",
    "dict_length=len(words)\n",
    "batch_size=4\n",
    "num_layers=1\n",
    "\n",
    "## Create labels\n",
    "Label=[]\n",
    "for caption in Tokenized:\n",
    "    Label+=[caption[1:]+[word_to_id['PAD']]]\n",
    "\n",
    "NumberofCasestoEvaluate=20\n",
    "TrainingInputs=Tokenized[:NumberofCasestoEvaluate]\n",
    "LabelInputs=Label[:NumberofCasestoEvaluate]\n",
    "\n",
    "#Print out some variables \n",
    "print(TrainingInputs[0])\n",
    "print(LabelInputs[0])\n",
    "print(\"Number of words in this dictionary \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our input queue\n",
    "def data_input_queue(TrainingInputs, LabelInputs, num_steps):\n",
    "    train_input_queue = tf.train.slice_input_producer(\n",
    "                                    [TrainingInputs, LabelInputs],\n",
    "                                    shuffle=True)\n",
    "\n",
    "    ##Set our train data and label input shape for the queue\n",
    "    TrainingInput=train_input_queue[0]\n",
    "    LabelInput=train_input_queue[1]\n",
    "    TrainingInput.set_shape([num_steps])\n",
    "    LabelInput.set_shape([num_steps])\n",
    "    min_after_dequeue=100000\n",
    "    capacity = min_after_dequeue + 3 * batch_size \n",
    "    #input_x, target_y\n",
    "    train_x, train_y = tf.train.batch([TrainingInput, LabelInput],\n",
    "                                                 batch_size=batch_size ,\n",
    "                                                 capacity=capacity,\n",
    "                                                 num_threads=4)\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to train an RNN with the MSCOCO captions. Before you start training define the dropout values you want to use for training. If you need a hint check [here](#answer3 \"Any value greater than 0 and less than or equal to 1 will work. For exampe input_keep_prob=1.0 and output_keep_prob=1.0 can be used\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 loss:  9.11365\n",
      "iteration:  100 loss:  0.596196\n",
      "iteration:  200 loss:  0.330111\n",
      "iteration:  300 loss:  0.195806\n",
      "iteration:  400 loss:  0.156069\n",
      "iteration:  500 loss:  0.166281\n",
      "iteration:  600 loss:  0.262709\n",
      "iteration:  700 loss:  0.0812796\n",
      "iteration:  800 loss:  0.174823\n",
      "iteration:  900 loss:  0.184339\n",
      "iteration:  1000 loss:  0.220383\n",
      "iteration:  1100 loss:  0.136037\n",
      "iteration:  1200 loss:  0.192143\n",
      "iteration:  1300 loss:  0.0695722\n",
      "iteration:  1400 loss:  0.145747\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "loss_mscoco=[]\n",
    "#######################################################################################################\n",
    "NumberofCasestoEvaluate=100\n",
    "TrainingInputs=Tokenized[:NumberofCasestoEvaluate]\n",
    "LabelInputs=Label[:NumberofCasestoEvaluate]\n",
    "\n",
    "\n",
    "## Make Variables\n",
    "# tf Graph input\n",
    "x = tf.placeholder(dtype=tf.int32, shape=(batch_size , num_steps))\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(batch_size,  num_steps))\n",
    "#tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "variables_dict = {\n",
    "    \"weights_mscoco\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32),name=\"weights_mscoco\"),\n",
    "    \"biases_mscoco\": tf.Variable(tf.truncated_normal([dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32), name=\"biases_mscoco\")}\n",
    "\n",
    "\n",
    "# Create input data\n",
    "train_x, train_y =data_input_queue(TrainingInputs, LabelInputs, num_steps)\n",
    "mscoco_dict=words\n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), x) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "y_one_hot=tf.unstack(tf.nn.embedding_lookup(np.identity(dict_length), y),num_steps,1) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "y_target_reshape=tf.reshape(y_one_hot,[batch_size*num_steps,dict_length])\n",
    "\n",
    "input_keep_prob=1.0\n",
    "output_keep_prob=1.0\n",
    "\n",
    "#Create a multilayer RNN\n",
    "\n",
    "layer_cell=[]\n",
    "for _ in range(num_layers):\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_hidden, state_is_tuple=True)\n",
    "    ############# add dropout #########################\n",
    "    lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell,\n",
    "                                          input_keep_prob=input_keep_prob,\n",
    "                                          output_keep_prob=output_keep_prob)\n",
    "    layer_cell.append(lstm_cell)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(layer_cell, state_is_tuple=True)\n",
    "outputs, last_states = tf.contrib.rnn.static_rnn(\n",
    "    cell=lstm_cell,\n",
    "    dtype=tf.float32,\n",
    "    inputs=tf.unstack(tf.to_float(X_one_hot),num_steps,1))\n",
    "\n",
    "output_reshape=tf.reshape(outputs, [batch_size*num_steps,num_hidden]) #[12==batch_size*num_steps,num_hidden==12]\n",
    "pred=tf.matmul(output_reshape, variables_dict[\"weights_mscoco\"]) +variables_dict[\"biases_mscoco\"]\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target_reshape))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(cost,aggregation_method = tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())    \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        for i in range(1500):\n",
    "            x_input,y_input=sess.run([train_x, train_y])\n",
    "            loss,_,y_target,y_pred=sess.run([cost,optimizer,y_target_reshape,pred],feed_dict={x:x_input,y:y_input})\n",
    "            loss_mscoco.append([loss])\n",
    "            if i% 100==0:\n",
    "                print(\"iteration: \",i, \"loss: \",loss)  \n",
    "        print(\"Done Training\")\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence\n",
      "[u'a', u'crisp', u'neutral', u'bathroom', u'embellished', u'with', u'modern', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Target\n",
      "[u'crisp', u'neutral', u'bathroom', u'embellished', u'with', u'modern', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted words\n",
      "[u'man', u'neutral', u'bathroom', u'embellished', u'with', u'modern', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "#Lets look at one input data point and its prediction\n",
    "print(\"Input Sentence\")\n",
    "batch_element=2\n",
    "print([words[ind] for ind in x_input[batch_element,:]])\n",
    "print(\"Target\")\n",
    "print([words[ind] for ind in y_input[batch_element,:]])\n",
    "print(\"Predicted words\")\n",
    "print([words[ind] for ind in np.argmax(y_pred[batch_element::batch_size],1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the single word prediction cell above represent a deployment scenario? \n",
    "\n",
    "No, the ground truth is being propagated through the network. The  predicted caption above represents a single predicted word based on the previous ground truth.\n",
    "\n",
    "In the classification part we had a function for our network. If you were going to make a function for this network, what would you include?  What would your inputs and outputs be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Free our GPU memory before proceeding to the next part of the lab\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References \n",
    "[1] Imanol Schlab. TensorFLow Input Pipeline Example. http://ischlag.github.io/2016/06/19/tensorflow-input-pipeline-example/\n",
    "\n",
    "[2] Denny Britz. Practical Examples for RNNs in TensorFlow https://github.com/dennybritz/tf-rnn\n",
    "\n",
    "[3]Lin, Tsung-Yi, et al. \"Microsoft coco: Common objects in context.\" European Conference on Computer Vision. Springer International Publishing, 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
